Daniel Kahneman and Amos Tversky’s research on the heuristic principles of decision-making jolted the psychologists of the 1970s and opened the world’s eyes to a surprising truth: Humans make irrational decisions. Prior to the duo’s work, scientists largely operated under the assumption that humans were “rational agents,” methodical and logical thinkers, prone to choosing the most optimal of two or more alternatives. But through decades of research, Kahneman and Tversky proved that humans, more often than not, take emotional shortcuts, leaving them vulnerable to all sorts of cognitive biases, which cloud decisions and lead to some pretty strange behaviors. Three major mishaps in U.S. history help demonstrate this principle well, reminding us that we can’t believe everything we see, or think. More important, these case studies from the past highlight what today’s patients and healthcare professionals stand to learn about irrational thinking: Seen And Ignored: The Story Of Sir Alexander Fleming In fall 1928, a Scottish-born physician and researcher named Alexander Fleming took off on a two-week vacation. Eager to get out the door and on the road, Fleming left his laboratory in London a bit unkempt. Upon his return, Fleming noticed a fungus had grown in an open petri dish that had been plated with the “staphylococcus” bacterium (the cause of staph infections). Had he been in a haste to clean up his messy lab, it would have been easy for Fleming to overlook the significance of what he saw. Thankfully, he didn’t. Looking closer, he observed a clear space between each white dot of fungus and the surrounding bacteria, indicating that the former was killing the latter. He later determined the fungus to be Penicillium notatum, containing an active substance we know today as “penicillin.” This story is well known to doctors and has inspired many to pursue groundbreaking research. But what’s most astonishing – and the major mishap of this tale – is how little the scientific community seemed to care about Fleming’s discovery, which was first published in the British Journal of Experimental Pathology in 1929. Imagine making an astonishing scientific breakthrough (perhaps the most significant in medical history) only to have your work go ignored for more than a decade. That’s precisely what happened to Sir Fleming. It wasn’t until World War II that scientists began to realize its full potential. In the decades since, millions of lives have been saved from the ravages of pneumonia, scarlet fever, gonorrhea, meningitis and other serious infections – all thanks to this wonder drug borne of a fungus. Unfortunately, this wasn’t history’s only medical innovation to be overlooked. Remember the story of Barry Marshall, another Nobel laureate? As a pathologist in Australia, Marshall identified a bacterium (H. pylori) that had a direct correlation with stomach ulcers. But, similar to Fleming, Marshall’s published results were dismissed for over a decade. And had he not received the Nobel Prize in 2005 (an announcement that shocked the scientific world), doctors might still be blaming stress and spicy food for stomach ulcers. Worse, they might still be referring ulcer patients to surgeons who would routinely “solve the problem” by removing two-thirds of a person’s stomach. Though much has changed in medical practice since Fleming and Marshall’s discoveries, one thing hasn’t. About 17 years ago, the Institute of Medicine estimated that a proven medical innovation takes an average of 17 years to make its way into routine patient care. Unfortunately, this delay continues to hold true to form. And because physicians still fail to implement best practices, hundreds of thousands of patients continue to suffer each year from risks identified decades ago, such as delays in care, medical errors and failures of prevention. Unseen Entirely: The Museum of Modern Art Henri Matisse was a pioneer of the Fauvist movement, a man who by some accounts revolutionized modern art. Near the end his life, in 1953, he created a paper-cut masterpiece titled Le Bateau. The Museum of Modern Art (MoMA) in New York City acquired and displayed “The Boat” in 1961 as part of a posthumous art exhibition, “The Last Works of Henri Matisse.” Art lovers instantly flocked to the gallery. For next 47 days, some 116,000 people passed by the artist’s whimsical interpretation of a sailboat. They marveled at its reflection dancing on the water against a backdrop of loosely styled clouds. And during those 47 days, no one – not one curator, critic or docent – noticed the piece had been hung upside down. In fact, the first to notice the error was Mrs. Genevieve Habert, a stockbroker from Paris. When asked how she’d figured out that MoMA had capsized Matisse’s boat, she explained that the artist “would never put the main, more complex motif on the bottom and the lesser motif at the top.” It’s amazing how much of the world around us goes unnoticed. The circumstances of our daily lives play an important role in this. For example, who’d ever think to question whether the boat in a Matisse paper-cut, hanging in one of the world’s most prestigious art museums, is right-side up or topsy-turvy? The same contextual blindness persists in medicine today. That is, within the context of a fee-for-service system that rewards doctors and hospitals for the volume – not the value – of care they deliver, doctors routinely perceive what they are doing as “right-side up,” even when scientific literature demonstrates the opposite. For instance, the most common treatment for a meniscus injury involves the use of an arthroscope. With it, surgeons look inside a patient’s knee before trimming and suturing the damaged cartilage. Interestingly, however, several large and well-controlled studies have shown that orthopedic surgery plus physical therapy is no better for most patients than physical therapy alone. But, given the context, this operative procedure still makes perfect sense to surgeons. The context in this case is a fee-for-service world is that of being paid 50-times to repair torn cartilage than to refer a patient to physical therapy. Long after objective data and research contradict a common medical practice, physicians continue to deliver it. The reason is simple. When a patient has a problem, and there’s a medical intervention for it, and the insurance company is willing to pay for it, physicians will see the procedure as appropriate. And they’ll do so regardless of whether a less expensive, more conservative alternative has proven equally successful. In a recent study, a dozen physicians from around the country reviewed all 363 articles published in the New England Journal of Medicine over the previous decade that were specific to medical practice. They found 146 studies that proved (or strongly suggested) a current standard practice either had no benefit or was inferior to the practice it replaced. Only 138 articles supported the efficacy of an existing practice. In most cases, clinical blindness was not due to a lack of information. Rather, it reflected our flawed reimbursement system. Seeing What Isn’t There: Three Mile Island In the 1970s, the global oil crisis threatened to destabilize the world economy. Seeking to gain independence from OPEC’s monopolistic production, the United States swiftly approved the construction of 47 nuclear reactors. One of them, located at Three Mile Island just south of Harrisburg in Pennsylvania, became the site of America’s most infamous industrial disaster. A partial core meltdown began just around 4 a.m. on March 28, 1979, the result of a misstep in routine maintenance that had taken place 11 hours earlier. The first indication that something was wrong came with a flash of warning lights and blare of horns in the control panel room belonging to the Unit II reactor. Though the control system had correctly diagnosed the problem, and thus initiated automatic safety devices to shut down the reactor, several humans intervened. What the workers saw was a computer error – one that didn’t exist. Thinking that the computer had misidentified the problem, plant operators manually overrode the controls. It would be another three hours before anyone recognized that contradicting the computers was a potentially fatal mistake. It wasn’t until 6:57 a.m., when the nuclear rods and radioactive material became exposed, that a plant supervisor realized the human error and declared the site an emergency area. The lesson here is not that computers are infallible, but that humans overrate their own intuition and ability. We not only tolerate personal preference and variation in workplace performance, we sometimes celebrate it. In medicine, this can have serious consequences. Nationwide, the rate of high-blood -pressure control is just 55% and yet the best medical groups achieve control rates of over 90%. The difference between the best and the rest? The best apply best practices. The rest reject algorithmic and machine-based recommendations as “cookbook medicine.” That is, instead of focusing on how often such approaches save lives, they harp on a minority of cases in which evidence-based care proved inaccurate. When we combine these errors in perception – physicians ignoring the problems they see, failing to see a problem entirely and seeing ones that aren’t there – we’re left with an estimated 12 million misdiagnoses every year. But like those who failed to make penicillin commercially available for over a decade, and those who failed to recognize that a Matisse was hung upside-down, and those who were certain their perception was better than the computer’s, Americans continue to tell themselves that we have the best healthcare in the world. The reality, according to the data, is that U.S. healthcare lags behind most industrialized countries in clinical outcomes. And our performance is unlikely to improve until we can fully see the shortcomings of our system. By accelerating the rate at which evidence-based approaches become standard practice, American physicians can eliminate many unnecessary procedures and patient deaths. And by eliminating financial incentives that encourage doctors to do more surgeries and procedures regardless of clinical efficacy, we can save our nation millions in excessive drug costs and thousands of unnecessary hospital visits. As it is when solving most problems, the first step is recognizing the problems exist. The second step is making sure the problems we see in healthcare are the right ones to solve. The final, and most important, step is taking action and making change happen. 