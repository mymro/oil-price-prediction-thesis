On 14 September 2019, a small number of ‘kamikaze’ drones hit the Khurais oil field and Abqaiq refinery in Saudi Arabia. Besides temporarily stalling production on 5% of the world’s crude oil, the attack highlighted some of the challenges associated with the use of these remotely controlled weapons.  First of all, the lack of accountability. Almost three months after the event, it’s still not clear who was responsible for the attack. Just like hackers in cyber warfare, the assailants left little or no trace behind them.  Another issue is that the small size and relatively low cost of these weapons could make them an appealing proposition to small groups of fighters and terrorists as well as nation-states, effectively ‘democratizing’ the destruction playing field.  Scary enough. Still, given the direction the arms industry is currently heading, these concerns could pale in comparison to those to come.  As Dutch NGO Pax puts it, in a report published a few weeks ago, we are on a slippery slope, running fast towards the adoption of weapons that are able to kill people without direct human involvement, the so-called Lethal Autonomous Weapons (LAWs), better known as ‘killer robots’.  The nickname is somehow misleading, evoking Terminator-like fantasies while, in fact, these tools resemble more traditional deadly devices: drones, tanks submarines; what changes is not the shape of the weapon, but the degree of autonomy involved. At the moment, fully autonomous offensive weapons do not seem to have been deployed on the ground, but machines are becoming increasingly sophisticated and less reliant on human control.  One such example is the Kargu ‘autonomous rotary wing attack drone’, developed by Turkish State-owned company STM, to be deployed by 2020 in Turkey’s eastern border with Syria. According to an article in the Turkish newspaper Hurriyet, each Kargu swarm will consist of 30 drones, all with artificial intelligence and facial recognition systems. Relying on the coordinates or images of the target, the Kargus will be able to do all by themselves, through what is described as their "fire-and-forget" capability. Luckily enough, the "abort mission" and "return home" options will still be available to human operators at all stages of the attack.   The T-14 Armata Russian tank, with its unmanned turret and computerized systems, is often quoted as another example of a machine that could soon become fully autonomous, although there are several doubts on whether this would actually make it more effective on the battlefield.  Pax’s report discusses the moral and legal issues stemming from the development of such technologies. A survey was sent to fifty of the largest or more advanced arms producers in the world, asking if they involved in the creation of increasingly autonomous weapons and which policy (if any) they had in place on that topic. Just eleven out of fifty companies responded to Pax’s survey, eight of which took the effort to answer the questions.  By using publicly available sources, the NGO was able nonetheless to classify all of them, distinguishing four companies as showing ‘best practice’, sixteen companies as being of ‘medium concern’ and thirty as of ‘high concern’.  All American largest arms producers: Lockheed Martin, Boeing and Raytheon (all US) fell in the latter category, together with Chinese companies AVIC and CASC, Israeli firms IAI, Elbit and Rafael, Russia’s Rostec and STM.  Not by chance, perhaps, Russia, Israel, and the US are among the countries that are strongly opposing a pre-emptive ban on killer robots which was discussed at the United Nations’ "Convention on Conventional Weapons" conference in Geneva in November (China takes a more nuanced, some might say more hypocritical approach, stating it opposes the use of LAWs, but not their production or commercialization).  Ethical concerns might be strong, but supporters of LAWs argue that they could actually save soldiers’ lives (if and when robots will fight only other robots), be more precise, avoiding casualties and stop unnecessary atrocities, performed by humans out of rage and need for revenge. Most of these selling points don’t hold to scrutiny, but they provide a convenient rationale for inaction.  There’s also plenty of money at stake.  By 2010, according to a study by Vincent Boulain and Maaike Verbruggern of Vrije Universiteit Brussel, the United States had already invested $4 billion into researching AWS with a further $18 billion earmarked for autonomy development through 2020.  Northeastern University’s professor Denise Garcia and Ph.D. candidate Justin Haner in their paper "The Artificial Intelligence Arms Race Trends and World Leaders in Autonomous Weapons Development" also point at China’s estimated annual budget of $250 billion for weapons’ development and to the country’s projected spending of $4.5 billion on drone technology by 2021.  With multi-billion dollar conglomerates on one side, and a group of activists and (mostly) developing States on the other, the fight to stop LAWs deployment might feel like an unequal struggle.  Still, as the New York Times noted in a recent feature, a pre-emptive ban has worked in the past, for laser blinding weapons and it’s theoretically possible that the same might happen for killer robots. Widespread public awareness and strong public support to the ban might make the difference, and that’s precisely what is missing at the moment. 